# Nutrition Supplement Data Analysis

This project focuses on analyzing a nutrition supplement dataset to identify influential predictors, effectively handle missing data and outliers, and enhance model interpretability. Through rigorous analysis and the use of multiple machine learning techniques, the project offers insights into key drivers of outcomes in the dataset, while also ensuring model transparency.

## Project Highlights
  * Regression Analysis: Performed detailed regression analysis to discover significant predictors of nutritional supplement effects, tackling missing data and outliers to improve model reliability.
  * Multivariate Models: Developed and evaluated a range of models, including:
    - Linear Regression
    - Tree-based models (e.g., Random Forest, XGBoost)
    - H2O Auto-ML for automated hyperparameter tuning and model selection
  * Model Explainability: Employed interpretability techniques such as:
    - SHAP (SHapley Additive exPlanations): Used SHAP values and visually appealing SHAP dependence plots to understand the influence of each feature on model predictions.
    - Feature Importance Analysis: Conducted using tree-based models and H2O Auto-ML to assess variable significance and hyperparameter importance.
  * Comprehensive Interpretability: Applied additional interpretability methods to further enhance understanding of model outputs, fostering more informed decision-making.
    
## Tools & Technologies
  * Python: For data analysis and model building.
  * Pandas, NumPy: Data manipulation and preprocessing.
  * Scikit-learn: Linear regression and tree-based models.
  * H2O Auto-ML: For automated machine learning and hyperparameter optimization.
  * SHAP Library: For model interpretability and visualizations.
    
## Results & Insights
  This project not only successfully identified influential variables in the nutrition supplement dataset but also offered a transparent, explainable framework to understand how each predictor impacts the outcome. The use of SHAP analysis and additional interpretability methods enhanced the transparency of machine learning models, making them more suitable for real-world decision-making.
